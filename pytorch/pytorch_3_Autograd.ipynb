{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动微分\n",
    "\n",
    "## 函数torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None)\n",
    "- tensors: 用于求导的张量，如 loss\n",
    "- retain_graph: 保存计算图。PyTorch 采用动态图机制，默认每次反向传播之后都会释放计算图。这里设置为 True 可以不释放计算图。\n",
    "- create_graph: 创建导数计算图，用于高阶求导\n",
    "- grad_tensors: 多梯度权重。当有多个 loss 混合需要计算梯度时，设置每个 loss 的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# retain_graph\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "# y=(x+w)*(w+1)\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "# 第一次求导，设置 retain_graph=True，保留计算图\n",
    "# 如果不设置retain_graph=True, 则第二次求导时候会报错，pytorch会默认删除构图\n",
    "y.backward(retain_graph=True)\n",
    "print(w.grad)\n",
    "# 第二次求导成功\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.])\n"
     ]
    }
   ],
   "source": [
    "# grad_tensors\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "\n",
    "y0 = torch.mul(a, b)    # y0 = (x+w) * (w+1)\n",
    "y1 = torch.add(a, b)    # y1 = (x+w) + (w+1)    dy1/dw = 2\n",
    "\n",
    "# 把两个 loss 拼接都到一起\n",
    "loss = torch.cat([y0, y1], dim=0)       # [y0, y1]\n",
    "# 设置两个 loss 的权重: y0 的权重是 1，y1 的权重是 2\n",
    "grad_tensors = torch.tensor([1., 2.])\n",
    "\n",
    "loss.backward(gradient=grad_tensors)    # gradient 传入 torch.autograd.backward()中的grad_tensors\n",
    "# 最终的 w 的导数由两部分组成。∂y0/∂w * 1 + ∂y1/∂w * 2\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要注意的\n",
    "# 在每次反向传播求导时，计算的梯度不会自动清零。如果进行多次迭代计算梯度而没有清零，那么梯度会在前一次的基础上叠加。\n",
    "\n",
    "# Example:\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "# 进行 4 次反向传播求导，每次最后都没有清零\n",
    "for i in range(4):\n",
    "    a = torch.add(w, x)\n",
    "    b = torch.add(w, 1)\n",
    "    y = torch.mul(a, b)\n",
    "    y.backward()\n",
    "    print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)\n",
    "\n",
    "- outputs: 用于求导的张量，如 loss\n",
    "- inputs: 需要梯度的张量\n",
    "- create_graph: 创建导数计算图，用于高阶求导\n",
    "- retain_graph:保存计算图\n",
    "- grad_outputs: 多梯度权重计算\n",
    "\n",
    "torch.autograd.grad()的返回结果是一个 tunple，需要取出第 0 个元素才是真正的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([6.], grad_fn=<MulBackward0>),)\n",
      "(tensor([2.]),)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.], requires_grad=True)\n",
    "y = torch.pow(x, 2)     # y = x**2\n",
    "# 如果需要求 2 阶导，需要设置 create_graph=True，让一阶导数 grad_1 也拥有计算图\n",
    "grad_1 = torch.autograd.grad(y, x, create_graph=True)   # grad_1 = dy/dx = 2x = 2 * 3 = 6\n",
    "print(grad_1)\n",
    "# 这里求 2 阶导\n",
    "grad_2 = torch.autograd.grad(grad_1[0], x)              # grad_2 = d(dy/dx)/dx = d(2x)/dx = 2\n",
    "print(grad_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73e03da126b73bfff3642ec5261d56fa25c444ea595de51041687efaa60dda41"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
