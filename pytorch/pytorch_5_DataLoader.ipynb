{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, world. 说什么随叫随到啊就这样的话就不要说了, by the way\n",
      "说 什 么 随 叫 随 到 啊 就 这 样 的 话 就 不 要 说 了    \n",
      "\n",
      " ▁HELLO▁WORLD  说什么随叫随到啊就这样的话就不要说了  ▁BY▁THE▁WAY \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# 计算\n",
    "import torch \n",
    "import numpy as np\n",
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "# 音频处理\n",
    "import soundfile as sf\n",
    "# 字符串处理\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import zhconv\n",
    "import sentencepiece as spm\n",
    "from zhon import hanzi\n",
    "\n",
    "\n",
    "class audioReader(object):\n",
    "    \"\"\"Audio model reader\n",
    "    \"\"\"\n",
    "    def __init__(self, dict_path, spm_model_path):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self._dict_path = dict_path\n",
    "        self._sp = spm.SentencePieceProcessor(spm_model_path)\n",
    "        self._sp.Load(spm_model_path)\n",
    "\n",
    "        self._dict_word2id = {}\n",
    "        self._dict_id2word = {}\n",
    "        \n",
    "    def _buid_dict(self):\n",
    "        \"\"\" build dict btw word and id\n",
    "        \"\"\"\n",
    "        with codecs.open(self._dict_path, \"r\", \"utf-8\") as dict_handle:\n",
    "            for tmp_line in dict_handle:\n",
    "                tmp_word, tmp_id = tmp_line.strip().split()\n",
    "                self._dict_id2word[tmp_id] = tmp_word\n",
    "                self._dict_word2id[tmp_word] = tmp_id\n",
    "\n",
    "    @staticmethod\n",
    "    def read_pcm(file_path, sample_rate = 16000):\n",
    "        \"\"\"read audio\n",
    "        \"\"\"\n",
    "        data, sample_rate = sf.read(file_path, samplerate = sample_rate, channels = 1, format=\"RAW\", subtype=\"PCM_16\")\n",
    "\n",
    "        return data, sample_rate\n",
    "\n",
    "    def _del_cn_spaces(self, text):\n",
    "        \"\"\"del chinese spaces\n",
    "        \"\"\"\n",
    "        pattern =re.compile(r'(?<=[\\u4e00-\\u9fa5])\\s+(?=[\\u4e00-\\u9fa5])')\n",
    "        out_text = pattern.sub(r'', text)\n",
    "        return out_text\n",
    "    \n",
    "    def _del_punc(self, text):\n",
    "        \"\"\"del punc\n",
    "        \"\"\"\n",
    "        # punctuation = r\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､　、〃〈〉《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·！？｡。\"\"\n",
    "        punctuation = string.punctuation + hanzi.punctuation\n",
    "        dicts = {i:'' for i in punctuation}\n",
    "        punc_table = str.maketrans(dicts)\n",
    "        out_text = text.translate(punc_table)\n",
    "        return out_text\n",
    "    \n",
    "    def _tra2simple(self, text):\n",
    "        \"\"\"trans traditional to simplified Chinese\n",
    "        \"\"\"\n",
    "        out_text = zhconv.convert(text, 'zh-cn')\n",
    "        return out_text      \n",
    "    \n",
    "    def _cn_encode(self, text, n_char = 1):\n",
    "        \"\"\"split chinese text by n_char\n",
    "        \"\"\"\n",
    "        n = n_char \n",
    "        text_split = [text[j : j + n] for j in range(0, len(text), n)]\n",
    "        text_flat = []\n",
    "        for tmp_chars in text_split:\n",
    "            text_flat.append(\"\".join(tmp_chars))\n",
    "        \n",
    "        cn_token_str = \" \".join(text_flat)\n",
    "        return cn_token_str\n",
    "    \n",
    "    def _en_snp(self, text):\n",
    "        \"\"\"split english text by spm model\n",
    "        \"\"\"\n",
    "        token = self._sp.EncodeAsPieces(text)\n",
    "        en_token_str = \"\".join(str(i) for i in token)\n",
    "        en_token_str = \" \" + en_token_str.strip() + \" \"\n",
    "\n",
    "        return en_token_str\n",
    "   \n",
    "    def _text2token(self, text):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        result_en = re.finditer(r'[a-z_A-Z-\\.!@#\\$%\\\\\\^&\\*\\)\\(\\+=\\{\\}\\[\\]\\/\",\\'<>~\\·`\\?:;][a-z_A-Z-\\.!@#\\$%\\\\\\^&\\*\\)\\(\\+=\\{\\}\\[\\]\\/\",\\'<>~\\·`\\?:;|\\s]*',text)\n",
    "        add_pos_en = 0\n",
    "        for i in result_en:\n",
    "            en_text = i.group().strip()\n",
    "            en_token = self._en_snp(en_text.upper())\n",
    "            start_pos = i.start() + add_pos_en\n",
    "            text = text[:start_pos] + text[start_pos:].replace(en_text, en_token, 1)\n",
    "            add_pos_en += (len(en_token)-len(en_text))\n",
    "\n",
    "        result_cn = re.finditer(r'([\\u4e00-\\u9fa5][\\u4e00-\\u9fa5\\s]*)',text)\n",
    "        add_pos_cn = 0\n",
    "        for j in result_cn:\n",
    "            cn_text = j.group()\n",
    "            cn_token = self._cn_encode(cn_text)\n",
    "            print(cn_token + \"\\n\")\n",
    "            strat_pos = j.start() + add_pos_cn\n",
    "            text = text[:start_pos] + text[start_pos:].replace(cn_text, cn_token, 1)\n",
    "            add_pos_cn += (len(cn_token)-len(cn_text))\n",
    "        return text\n",
    "\n",
    "    def _sym2id(self, token):\n",
    "        \"\"\"trans token 2 id, base on self dict\n",
    "        \"\"\"\n",
    "        token_list = token.strip().split()\n",
    "        for i in range(len(token_list)):\n",
    "            char_token = token_list[i]\n",
    "            try:\n",
    "                token_list[i] = self._word_map[char_token]\n",
    "            except Exception as e:\n",
    "                logging.error(token +  \"\\t\" + token_list[i]+\" is replace 1 \")\n",
    "                token_list[i] = \"1\"\n",
    "\n",
    "        tokenid = \" \".join(token_list)\n",
    "        return tokenid\n",
    "\n",
    "    def trans_char2id(self, text):\n",
    "        \"\"\"trans character 2 index\n",
    "        \"\"\"\n",
    "        text = self._del_cn_spaces(text)\n",
    "        text = self._del_punc(text)\n",
    "        text = self._tra2simple(text)\n",
    "        seq_ids = self._text2token(text)\n",
    "        \n",
    "        return seq_ids\n",
    "\n",
    "    def read_pcm_text(input_line):\n",
    "        \"\"\"read pcm and text, split by \\t or space\n",
    "        \"\"\"\n",
    "        line = input_line.strip()\n",
    "        pcm_path, text = line.split()\n",
    "\n",
    "        data, sample_rate = self.read(pcm_path)\n",
    "        \n",
    "\n",
    "dict_file = \"../Data/cnen_dict_14323_units.txt\"\n",
    "smp_model = \"../Data/cnen_spm_unigram5000.model\"\n",
    "test_text = \"hello, world. 说什么随叫随到啊就这样的话就不要说了, by the way\"\n",
    "\n",
    "test_reader = audioReader(dict_file, smp_model)\n",
    "\n",
    "test_text_ids = test_reader.trans_char2id(test_text)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca046b47b406f7b30aefed738e5e187fb84305d2c79705ba3314c8947ed88f9f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
