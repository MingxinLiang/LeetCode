{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ▁HELLO ▁WORLD \n",
      " ▁BY ▁THE ▁WAY \n",
      "id_seq: 11379 14268 7467 124 57 8594 835 8594 603 1103 2012 7885 3563 5200 7445 2012 12 7287 7467 86 9987 13770 14155\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# 计算\n",
    "import torch \n",
    "import numpy as np\n",
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "# 音频处理\n",
    "import soundfile as sf\n",
    "# 字符串处理\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import zhconv\n",
    "import sentencepiece as spm\n",
    "from zhon import hanzi\n",
    "# log\n",
    "import logging\n",
    "\n",
    "\n",
    "class audioReader(object):\n",
    "    \"\"\"Audio model reader\n",
    "    \"\"\"\n",
    "    def __init__(self, dict_path = None, spm_model_path = None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if spm_model_path is None:\n",
    "            logging.warn(\"Spm model is not set.\"))\n",
    "        else:\n",
    "            self._sp = spm.SentencePieceProcessor(spm_model_path)\n",
    "            self._sp.Load(spm_model_path)\n",
    "            \n",
    "\n",
    "        if dict_path is None:\n",
    "            logging.warn(\"Word 2 id dict is not set.\"))\n",
    "        else:\n",
    "            self._dict_word2id = {}\n",
    "            self._dict_id2word = {}\n",
    "            self._dict_path = dict_path\n",
    "            self._buid_dict()\n",
    "\n",
    "    def _buid_dict(self):\n",
    "        \"\"\" build dict btw word and id\n",
    "        \"\"\"\n",
    "        with codecs.open(self._dict_path, \"r\", \"utf-8\") as dict_handle:\n",
    "            for tmp_line in dict_handle:\n",
    "                tmp_word, tmp_id = tmp_line.strip().split()\n",
    "                self._dict_id2word[tmp_id] = tmp_word\n",
    "                self._dict_word2id[tmp_word] = tmp_id\n",
    "\n",
    "    @staticmethod\n",
    "    def read_pcm(file_path, sample_rate = 16000):\n",
    "        \"\"\"read audio\n",
    "        \"\"\"\n",
    "        data, sample_rate = sf.read(file_path, samplerate = sample_rate, channels = 1, format=\"RAW\", subtype=\"PCM_16\")\n",
    "\n",
    "        return data, sample_rate\n",
    "\n",
    "    def _del_cn_spaces(self, text):\n",
    "        \"\"\"del chinese spaces\n",
    "        \"\"\"\n",
    "        pattern =re.compile(r'(?<=[\\u4e00-\\u9fa5])\\s+(?=[\\u4e00-\\u9fa5])')\n",
    "        out_text = pattern.sub(r'', text)\n",
    "        return out_text\n",
    "    \n",
    "    def _del_punc(self, text):\n",
    "        \"\"\"del punc\n",
    "        \"\"\"\n",
    "        # punctuation = r\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､　、〃〈〉《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏﹑﹔·！？｡。\"\"\n",
    "        punctuation = string.punctuation + hanzi.punctuation\n",
    "        dicts = {i:'' for i in punctuation}\n",
    "        punc_table = str.maketrans(dicts)\n",
    "        out_text = text.translate(punc_table)\n",
    "        return out_text\n",
    "    \n",
    "    def _tra2simple(self, text):\n",
    "        \"\"\"trans traditional to simplified Chinese\n",
    "        \"\"\"\n",
    "        out_text = zhconv.convert(text, 'zh-cn')\n",
    "        return out_text      \n",
    "    \n",
    "    def _cn_encode(self, text, n_char = 1):\n",
    "        \"\"\"split chinese text by n_char\n",
    "        \"\"\"\n",
    "        n = n_char \n",
    "        text_split = [text[j : j + n] for j in range(0, len(text), n)]\n",
    "        text_flat = []\n",
    "        for tmp_chars in text_split:\n",
    "            text_flat.append(\"\".join(tmp_chars))\n",
    "        \n",
    "        cn_token_str = \" \".join(text_flat)\n",
    "        return cn_token_str\n",
    "    \n",
    "    def _en_snp(self, text):\n",
    "        \"\"\"split english text by spm model\n",
    "        \"\"\"\n",
    "        token = self._sp.EncodeAsPieces(text)\n",
    "        en_token_str = \" \".join(str(i) for i in token)\n",
    "        en_token_str = \" \" + en_token_str.strip() + \" \"\n",
    "\n",
    "        return en_token_str\n",
    "   \n",
    "    def _text2token(self, text):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        result_en = re.finditer(r'[a-z_A-Z-\\.!@#\\$%\\\\\\^&\\*\\)\\(\\+=\\{\\}\\[\\]\\/\",\\'<>~\\·`\\?:;][a-z_A-Z-\\.!@#\\$%\\\\\\^&\\*\\)\\(\\+=\\{\\}\\[\\]\\/\",\\'<>~\\·`\\?:;|\\s]*',text)\n",
    "        add_pos_en = 0\n",
    "        for i in result_en:\n",
    "            en_text = i.group().strip()\n",
    "            en_token = self._en_snp(en_text.upper())\n",
    "            print(en_token)\n",
    "            start_pos = i.start() + add_pos_en\n",
    "            text = text[:start_pos] + text[start_pos:].replace(en_text, en_token, 1)\n",
    "            add_pos_en += (len(en_token)-len(en_text))\n",
    "\n",
    "        result_cn = re.finditer(r'([\\u4e00-\\u9fa5][\\u4e00-\\u9fa5\\s]*)',text)\n",
    "        add_pos_cn = 0\n",
    "        for j in result_cn:\n",
    "            cn_text = j.group()\n",
    "            cn_token = self._cn_encode(cn_text)\n",
    "            start_pos = j.start() + add_pos_cn\n",
    "            text = text[:start_pos] + text[start_pos:].replace(cn_text, cn_token, 1)\n",
    "            add_pos_cn += (len(cn_token)-len(cn_text))\n",
    "        return text\n",
    "\n",
    "    def _sym2id(self, token):\n",
    "        \"\"\"trans token 2 id, base on self dict.\n",
    "        \"\"\"\n",
    "        token_list = token.strip().split()\n",
    "        for i in range(len(token_list)):\n",
    "            char_token = token_list[i]\n",
    "            try:\n",
    "                token_list[i] = self._dict_word2id[char_token]\n",
    "            except Exception as e:\n",
    "                logging.error(token +  \"\\t\" + token_list[i]+\" is replace 1 \")\n",
    "                token_list[i] = \"1\"\n",
    "\n",
    "        token_id = \" \".join(token_list)\n",
    "        return token_id\n",
    "\n",
    "    def trans_char2id(self, text):\n",
    "        \"\"\"trans character 2 index\n",
    "        \"\"\"\n",
    "        text = self._del_cn_spaces(text)\n",
    "        text = self._del_punc(text)\n",
    "        text = self._tra2simple(text)\n",
    "        token_seq = self._text2token(text)\n",
    "\n",
    "        id_seq = self._sym2id(token_seq)\n",
    "        \n",
    "        return id_seq\n",
    "\n",
    "    def read_pcm_text(self, input_line):\n",
    "        \"\"\"read pcm and text, split by \\t or space\n",
    "        \"\"\"\n",
    "        line = input_line.strip()\n",
    "        pcm_path, text = line.split()\n",
    "\n",
    "        data, sample_rate = self.read(pcm_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ▁HELLO ▁WORLD \n",
      " ▁BY ▁THE ▁WAY \n",
      "id_seq: 11379 14268 7467 124 57 8594 835 8594 603 1103 2012 7885 3563 5200 7445 2012 12 7287 7467 86 9987 13770 14155\n"
     ]
    }
   ],
   "source": [
    "dict_file = \"../Data/cnen_dict_14323_units.txt\"\n",
    "smp_model = \"../Data/cnen_spm_unigram5000.model\"\n",
    "test_text = \"hello, world. 说什么随叫随到啊就这样的话就不要说了, by the way\"\n",
    "\n",
    "test_reader = audioReader(dict_file, smp_model)\n",
    "\n",
    "test_text_ids = test_reader.trans_char2id(test_text)\n",
    "print(\"id_seq:\", test_text_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca046b47b406f7b30aefed738e5e187fb84305d2c79705ba3314c8947ed88f9f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
